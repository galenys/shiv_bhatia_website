<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="https://fonts.googleapis.com/css?family=Oxanium&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/main.css">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Shiv Bhatia - Word2Vec</title>
  </head>
  <body>
    <div class="header">
      <a href="/index.html" class="home">Home</a>
      <a href="/blog.html">Blog</a>
      <a href="/shiv_bhatia_cv.pdf">CV</a>
      <a href="https://www.youtube.com/channel/UChVjUEFdA3JLxANpOH-t_fQ">Music</a>
    </div>

    <div class="content">
      <h1>The magic of word embeddings in modern NLP</h1>
      <p class="subtitle">Jan 14, 2021</p>
      <p>
      How do computers represent words? You probably know that strings are held in memory as collections of characters, which are in turn usually 8-bit binary numbers that are assigned to specific letters and symbols. 
      <br><br>
      That's all well and good if you're trying to write in a word processor or compile some code. But how the hell are you supposed to train a neural network with an array of bytes?
      <br><br>
      For one thing, words are of variable length. For another, the precise order of letters in a word often has nothing to do with its meaning (think of 'horse' and 'hoarse'). Machine learning algorithms play well with vectors, and having some sort of mapping between words and high dimensional vectors seems like it could lend itself to capturing meaning and context pretty well. But how would you even begin to do that?
      <br><br>
      In this post, I'm going to explain one of the most popular algorithms for word embedding, Word2Vec. It's the origin of the famous "king" - "man" + "woman" = "queen" equation. I'm not going to aim for thoroughness; I recommend <a href="https://jalammar.github.io/illustrated-word2vec/">this excellent post</a> by Jay Alammar if you want a deeper explanation. My goal is for you to have a high-level understanding of how word embeddings work. The ideas here apply in many, many other domains of machine learning and beyond.
      <br><br>
      So without further ado, here is a summary of the key ideas behind Word2Vec and others like it:
      <br><br>
      <b>IDEA 1: Words can derive meaning from the words around them.</b> For every word in the corpus you're training on, create an entry in your training data of that word paired with the 4 words closest to it (2 on either side). You can imagine this as a sliding window of 5 words, where the word in the middle is the input and the rest are the outputs (the order is sometimes switched around, but that's not super important for us right now). As an example, from the sentence "The quick brown fox jumped over the lazy dog", you might derive the examples [(fox, quick), (fox, brown), (fox, jumped), (fox, over)]. This will become our training set. Note that we now have 4 entries for every word in the corpus. That's a lot of training data!
      <br><br>
      <b>IDEA 2: Create a hash map between the words in your vocabulary and the vectors that represent them.</b> Or a dictionary, as Python people call it. Initially, the vectors will be randomised. They're usually around 50-dimensional, though this can vary depending on how much nuance you need in your model. You actually need two seperate dictionaries, one for the embedding vectors and one for the context vectors. You can think of these hash maps as matrices of dimension (length of one vector) * (number of words in vocabulary).
      <br><br>
      <b>IDEA 3: Introduce some counter examples, known as negative sampling.</b> This is speeds up training significantly. Just take random pairs of words from your vocabulary and stick them into the training set. This means we also need to have a third column in the database, which is either 1 or 0 depending on whether the words should actually be together or not. Let's call it the similarity classification: it will be useful for calculating the error.
      <br><br>
      <b>IDEA 4: Take the dot product between your embedding and context vector pairs, pass it through a sigmoid function, and compare it to the similarity classification.</b> The dot product, as you may know, measures in part how much two vectors point in the same direction. Now we start to see the geometry of this whole thing; we're basically pushing these words that are close to each other in our corpus closer to each other in our vector embedding space. The sigmoid clamps the values between 0 and 1, and then we just take the mean squared difference between this value and the similarity classification. This is our error.
      <br><br>
      <b>IDEA 5: Just gradient descent that shit.</b> Even though there's nothing here that looks like a neural network, we can still use gradient descent to optimise the embeddings.
      <br><br>
      And that's basically it! You might have noticed that we end up with two seperate vector embeddings, one for each hash map we used. You can use either one; we only needed two for training purposes. You now have a basic understanding of how word embeddings are created, even if you don't know the nitty-gritty details.
      <br><br>
      These vectors can be added and subtracted to figure out proportional meanings, something which I find insanely cool. My personal favourite example that Word2Vec came up with is "indifference" - "love" + "fear" = "apathy". Pretty deep stuff.
      <br><br>
      You're now ready to use word embeddings in your natural language processing models! Have fun, and try not to let the eerily prescient knowledge the vectors seem to possess freak you out too much.
      </p>
    </div>
  </body>
</html>
